{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ текстов на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import pymorphy3\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>tag</th>\n",
       "      <th>value</th>\n",
       "      <th>pstv</th>\n",
       "      <th>ngtv</th>\n",
       "      <th>neut</th>\n",
       "      <th>dunno</th>\n",
       "      <th>pstvNgtvDisagreementRatio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>абажур</td>\n",
       "      <td>NEUT</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>аббатство</td>\n",
       "      <td>NEUT</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>аббревиатура</td>\n",
       "      <td>NEUT</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>абзац</td>\n",
       "      <td>NEUT</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>абиссинец</td>\n",
       "      <td>NEUT</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           term   tag  value   pstv   ngtv   neut  dunno  \\\n",
       "0        абажур  NEUT   0.08  0.185  0.037  0.580  0.198   \n",
       "1     аббатство  NEUT   0.10  0.192  0.038  0.578  0.192   \n",
       "2  аббревиатура  NEUT   0.08  0.196  0.000  0.630  0.174   \n",
       "3         абзац  NEUT   0.00  0.137  0.000  0.706  0.157   \n",
       "4     абиссинец  NEUT   0.28  0.151  0.113  0.245  0.491   \n",
       "\n",
       "   pstvNgtvDisagreementRatio  \n",
       "0                       0.00  \n",
       "1                       0.00  \n",
       "2                       0.00  \n",
       "3                       0.00  \n",
       "4                       0.19  "
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"/Users/chervonikov_alexey/Desktop/projects/Technopark_Spring_2025/diploma_project/Purify/purify_ml/data/raw/kartaslovsent.csv\", sep=\";\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 46127\n",
      "Пример записи: ('абажур', ['NEUT', 0.08])\n",
      "Слово 'сопляк' имеет метку: ['NGTV', -1.0]\n"
     ]
    }
   ],
   "source": [
    "# term_to_tag_dict = dict(zip(dataset['term'], (dataset['tag'], dataset['ngtv'])))\n",
    "term_to_tag_dict = {row['term']: [row['tag'], row['value']] for _, row in dataset.iterrows()}\n",
    "\n",
    "print(\"Размер словаря:\", len(term_to_tag_dict))\n",
    "print(\"Пример записи:\", list(term_to_tag_dict.items())[0])\n",
    "\n",
    "test_word = \"сопляк\"\n",
    "if test_word in term_to_tag_dict:\n",
    "    print(f\"Слово '{test_word}' имеет метку: {term_to_tag_dict[test_word]}\")\n",
    "else:\n",
    "    print(f\"Слово '{test_word}' отсутствует в словаре\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEUT 0.31\n",
      "NEUT\n"
     ]
    }
   ],
   "source": [
    "tag, prob = term_to_tag_dict.get(\"кончить\", [\"NEUT\", 0])\n",
    "print(tag, prob)\n",
    "if tag == 'NGTV' and prob < -0.95:\n",
    "    tag = \"NGTV\"\n",
    "else:\n",
    "\ttag = \"NEUT\"\n",
    "\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бордель\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = pymorphy3.MorphAnalyzer()\n",
    "lemma = lemmatizer.parse(\"бордель\")[0].normal_form\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регулярка на мат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_regex = r\"\"\"(?iux)(?<![а-яё])(?:\n",
    "(?:(?:у|[нз]а|(?:хитро|не)?вз?[ыьъ]|с[ьъ]|(?:и|ра)[зс]ъ?|(?:о[тб]|п[оа]д)[ьъ]?|(?:\\S(?=[а-яё]))+?[оаеи-])-?)?(?:\n",
    "  [её](?:б(?!о[рй]|рач)|п[уа](?:ц|тс))|\n",
    "  и[пб][ае][тцд][ьъ]\n",
    ").*?|\n",
    "\n",
    "(?:(?:н[иеа]|(?:ра|и)[зс]|[зд]?[ао](?:т|дн[оа])?|с(?:м[еи])?|а[пб]ч|в[ъы]?|пр[еи])-?)?ху(?:[яйиеёю]|л+и(?!ган)).*?|\n",
    "\n",
    "бл(?:[эя]|еа?)(?:[дт][ьъ]?)?|\n",
    "\n",
    "\\S*?(?:\n",
    "  п(?:\n",
    "    [иеё]зд|\n",
    "    ид[аое]?р|\n",
    "    ед(?:р(?!о)|[аое]р|ик)|\n",
    "    охую\n",
    "  )|\n",
    "  бля(?:[дбц]|тс)|\n",
    "  [ое]ху[яйиеё]|\n",
    "  хуйн\n",
    ").*?|\n",
    "\n",
    "(?:о[тб]?|про|на|вы)?м(?:\n",
    "  анд(?:[ауеыи](?:л(?:и[сзщ])?[ауеиы])?|ой|[ао]в.*?|юк(?:ов|[ауи])?|е[нт]ь|ища)|\n",
    "  уд(?:[яаиое].+?|е?н(?:[ьюия]|ей))|\n",
    "  [ао]л[ао]ф[ьъ](?:[яиюе]|[еёо]й)\n",
    ")|\n",
    "\n",
    "елд[ауые].*?|\n",
    "ля[тд]ь|\n",
    "(?:[нз]а|по)х\n",
    ")(?![а-яё])\"\"\"\n",
    "\n",
    "PRONOUNS = ['я', 'ты', 'вы', 'он', 'она', 'оно', 'мы', 'они', 'вас', 'нас', 'их', 'его', 'её']\n",
    "stopword_set = set(nltk.corpus.stopwords.words('russian'))\n",
    "stopword_set = stopword_set.union({'это', 'который', 'весь', 'наш', 'свой', 'ещё', 'её', 'ваш', 'также', 'итак'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идем в лемматизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = pymorphy3.MorphAnalyzer()\n",
    "lemma = lemmatizer.parse(word)[0].normal_form\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def get_lemma(word):\n",
    "    return lemmatizer.parse(word)[0].normal_form\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = ' '.join(text.split())\n",
    "    return text.lower()\n",
    "\n",
    "def is_pronoun_or_stopword(word):\n",
    "    return word in PRONOUNS or word in stopword_set\n",
    "\n",
    "def split_compound_words(word):\n",
    "    return re.findall(r'[А-Яа-яё]+', word)\n",
    "\n",
    "word_cache = {}\n",
    "\n",
    "def get_negative_words(text):\n",
    "    cleaned_text = clean_text(text)\n",
    "    print(cleaned_text)\n",
    "    negative_words = set()\n",
    "    mat_words = list(re.findall(mat_regex, cleaned_text, re.VERBOSE))\n",
    "    mat_words = set(split_compound_words(\"_\".join(mat_words)))\n",
    "    words = split_compound_words(text)\n",
    "    print(words)\n",
    "    for word in words:\n",
    "        if is_pronoun_or_stopword(word):\n",
    "            continue\n",
    "\n",
    "        if word in word_cache:\n",
    "            if word_cache[word] == 'NGTV':\n",
    "                negative_words.add(word)\n",
    "            continue\n",
    "\n",
    "        if word in mat_words:\n",
    "            word_cache[word] = \"NGTV\"\n",
    "            negative_words.add(word)\n",
    "            continue\n",
    "\n",
    "        if word == \"сука\":\n",
    "            word_cache[word] = \"NGTV\"\n",
    "            negative_words.add(word)\n",
    "            continue\n",
    "\n",
    "        lemma = get_lemma(word)\n",
    "        \n",
    "        if is_pronoun_or_stopword(lemma):\n",
    "            continue\n",
    "        \n",
    "        if lemma in term_to_tag_dict:\n",
    "            tag = term_to_tag_dict[lemma]\n",
    "            word_cache[word] = tag\n",
    "            if tag == 'NGTV':\n",
    "                negative_words.add(word)\n",
    "        else:\n",
    "            word_cache[word] = 'NEUT'\n",
    "    \n",
    "    return list(negative_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "который год - в стране царствует смута и развал, властвует произвол финансово- чиновничьей олигархии. который год мы ожидаем обещанного благополучия и процветания, получая взамен безудержный рост цен, неплатежи по зарплатам и социальным пособиям, межнациональные войны и конфликты, бандитизм и коррупцию. довольно слушать бесконечные обещания и заверения чиновников, терпеть унижения и издевательства обнаглевших \"реформаторов\". на попытку ельцина, с помощью отставки правительства, уйти от ответственности за содеянное - ответим решительным: ельцина - в отставку ! на попытку ельцина путем политических рокировок продлить агонию ненавистного антинародного режима - ответим : нет - антинародному курсу !на угрозы президента распустить государственную думу, выступающую за изменение курса \"реформ \", заявим: руки прочь от государственной думы! даешь правительство народного доверия!\n",
      "['Который', 'год', 'в', 'стране', 'царствует', 'смута', 'и', 'развал', 'властвует', 'произвол', 'финансово', 'чиновничьей', 'олигархии', 'Который', 'год', 'мы', 'ожидаем', 'обещанного', 'благополучия', 'и', 'процветания', 'получая', 'взамен', 'безудержный', 'рост', 'цен', 'неплатежи', 'по', 'зарплатам', 'и', 'социальным', 'пособиям', 'межнациональные', 'войны', 'и', 'конфликты', 'бандитизм', 'и', 'коррупцию', 'Довольно', 'слушать', 'бесконечные', 'обещания', 'и', 'заверения', 'чиновников', 'терпеть', 'унижения', 'и', 'издевательства', 'обнаглевших', 'реформаторов', 'На', 'попытку', 'Ельцина', 'с', 'помощью', 'отставки', 'правительства', 'уйти', 'от', 'ответственности', 'за', 'содеянное', 'ответим', 'решительным', 'Ельцина', 'в', 'отставку', 'На', 'попытку', 'Ельцина', 'путем', 'политических', 'рокировок', 'продлить', 'агонию', 'ненавистного', 'антинародного', 'режима', 'ответим', 'НЕТ', 'антинародному', 'курсу', 'На', 'угрозы', 'президента', 'распустить', 'Государственную', 'Думу', 'выступающую', 'за', 'изменение', 'курса', 'реформ', 'заявим', 'Руки', 'прочь', 'от', 'Государственной', 'Думы', 'Даешь', 'Правительство', 'народного', 'доверия']\n",
      "Негативные слова: ['антинародного', 'чиновничьей', 'неплатежи', 'смута', 'олигархии', 'издевательства', 'произвол', 'развал', 'антинародному', 'унижения', 'агонию', 'войны', 'бандитизм', 'обнаглевших', 'отставку', 'угрозы', 'властвует', 'коррупцию', 'ненавистного', 'терпеть', 'конфликты', 'отставки']\n"
     ]
    }
   ],
   "source": [
    "text = '''Который год - в стране царствует смута и развал, властвует произвол финансово- чиновничьей олигархии.\n",
    "Который год мы ожидаем обещанного благополучия и процветания, получая взамен безудержный рост цен, неплатежи по зарплатам и социальным пособиям, межнациональные войны и конфликты, бандитизм и коррупцию.\n",
    "Довольно слушать бесконечные обещания и заверения чиновников, терпеть унижения и издевательства обнаглевших \"реформаторов\".\n",
    "На попытку Ельцина, с помощью отставки правительства, уйти от ответственности за содеянное - ответим решительным: Ельцина - в отставку !\n",
    "На попытку Ельцина путем политических рокировок продлить агонию ненавистного антинародного режима - ответим : НЕТ - антинародному курсу !На угрозы президента распустить Государственную Думу, выступающую за изменение курса \"реформ \", заявим: Руки прочь от Государственной Думы! Даешь Правительство народного доверия!'''\n",
    "negative_words = get_negative_words(text)\n",
    "print(\"Негативные слова:\", negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исправление ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CORRECT</th>\n",
       "      <th>MISTAKE</th>\n",
       "      <th>WEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>болота</td>\n",
       "      <td>балото</td>\n",
       "      <td>0.2652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>болота</td>\n",
       "      <td>боллото</td>\n",
       "      <td>0.0909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>болота</td>\n",
       "      <td>болотоэ</td>\n",
       "      <td>0.0909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>болото</td>\n",
       "      <td>палатаа</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>болото</td>\n",
       "      <td>болотл</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CORRECT  MISTAKE  WEIGHT\n",
       "1  болота   балото  0.2652\n",
       "2  болота  боллото  0.0909\n",
       "3  болота  болотоэ  0.0909\n",
       "4  болото  палатаа  0.5000\n",
       "5  болото   болотл  0.3333"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_dataset = pd.read_csv(\"/Users/chervonikov_alexey/Desktop/projects/Technopark_Spring_2025/diploma_project/Purify/purify_ml/data/raw/orfo_and_typos.L1_5+PHON.csv\", sep = \";\").iloc[1:, :]\n",
    "# errors_dataset['weight'] = errors_dataset['weight'].apply(lambda x: '{0:.15f}'.format(float(x)))\n",
    "errors_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string_weights = errors_dataset[~errors_dataset['weight'].apply(lambda x: str(x).replace('.', '').isdigit())]\n",
    "# string_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 90971 entries, 1 to 90971\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CORRECT  90971 non-null  object \n",
      " 1   MISTAKE  90971 non-null  object \n",
      " 2   WEIGHT   90971 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "errors_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02596902847290039\n",
      "None\n",
      "0.021033048629760742\n",
      "0.022637128829956055\n",
      "0.022774934768676758\n",
      "0.02228713035583496\n",
      "Это тесты текст с ашибками и опечатками некрасиво.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from functools import lru_cache\n",
    "\n",
    "class SpellChecker:\n",
    "    def __init__(self, dataset_path, max_distance=2):\n",
    "        self.correct_words = set()\n",
    "        self.error_to_correct = defaultdict(list)\n",
    "        self.max_distance = max_distance\n",
    "        \n",
    "        df = pd.read_csv(dataset_path, sep=';', header=None, \n",
    "                        names=['correct', 'error', 'weight']).iloc[1:, :]\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            correct = row['correct'].strip().lower()\n",
    "            error = row['error'].strip().lower()\n",
    "            weight = float(row['weight'])\n",
    "            \n",
    "            self.correct_words.add(correct)\n",
    "            self.error_to_correct[error].append((correct, weight))\n",
    "        \n",
    "        for error in self.error_to_correct:\n",
    "            self.error_to_correct[error].sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        self.all_known_words = list(self.correct_words) + list(self.error_to_correct.keys())\n",
    "    \n",
    "    @lru_cache(maxsize=10000)\n",
    "    def find_closest_word(self, word):\n",
    "        if not word:\n",
    "            return None\n",
    "            \n",
    "        word = word.lower()\n",
    "        \n",
    "        if word in self.correct_words:\n",
    "            return word\n",
    "        if word in self.error_to_correct:\n",
    "            return self.error_to_correct[word][0][0]\n",
    "        \n",
    "        min_distance = float('inf')\n",
    "        closest_word = None\n",
    "        \n",
    "        for known_word in self.all_known_words:\n",
    "            current_distance = levenshtein_distance(word, known_word)\n",
    "            if current_distance < min_distance and current_distance <= self.max_distance:\n",
    "                min_distance = current_distance\n",
    "                closest_word = known_word\n",
    "        \n",
    "        return closest_word\n",
    "    \n",
    "    def correct_spelling(self, word):\n",
    "        word = word.lower().strip()\n",
    "        \n",
    "        if hasattr(self, '_spelling_cache') and word in self._spelling_cache:\n",
    "            return self._spelling_cache[word]\n",
    "        \n",
    "        if word in self.correct_words:\n",
    "            return word\n",
    "        \n",
    "        if word in self.error_to_correct:\n",
    "            correction = self.error_to_correct[word][0][0]\n",
    "            if not hasattr(self, '_spelling_cache'):\n",
    "                self._spelling_cache = {}\n",
    "            self._spelling_cache[word] = correction\n",
    "            return correction\n",
    "        \n",
    "        import time\n",
    "        start = time.time()\n",
    "        closest_word = self.find_closest_word(word)\n",
    "        end = time.time()\n",
    "        print(f\"{end - start}\")\n",
    "        if closest_word:\n",
    "            if closest_word in self.error_to_correct:\n",
    "                correction = self.error_to_correct[closest_word][0][0]\n",
    "            else:\n",
    "                correction = closest_word\n",
    "            \n",
    "            if not hasattr(self, '_spelling_cache'):\n",
    "                self._spelling_cache = {}\n",
    "            self._spelling_cache[word] = correction\n",
    "            return correction\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def correct_text(self, text):\n",
    "        tokens = re.findall(r\"(\\w+|\\W+)\", text)\n",
    "        corrected_tokens = []\n",
    "        for token in tokens:\n",
    "            if token.strip() and token[0].isalpha():  \n",
    "                correction = self.correct_spelling(token)\n",
    "                if correction is not None:\n",
    "                    if token[0].isupper():\n",
    "                        correction = correction.capitalize()\n",
    "                    corrected_tokens.append(correction)\n",
    "                else:\n",
    "                    corrected_tokens.append(token)\n",
    "            else:\n",
    "                corrected_tokens.append(token)\n",
    "        \n",
    "        return ''.join(corrected_tokens)\n",
    "\n",
    "checker = SpellChecker(\"/Users/chervonikov_alexey/Desktop/projects/Technopark_Spring_2025/diploma_project/Purify/purify_ml/data/raw/orfo_and_typos.L1_5+PHON.csv\")\n",
    "print(checker.correct_spelling(\"оборзели\"))\n",
    "text = \"Это тествый текст с ашибками и опечатками некрасыво.\"\n",
    "print(checker.correct_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05901384353637695\n",
      "оконченный\n"
     ]
    }
   ],
   "source": [
    "print(checker.correct_spelling(\"конченный\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "Ты тупкя!\n"
     ]
    }
   ],
   "source": [
    "print(checker.correct_spelling(\"туптй\"))  \n",
    "print(checker.correct_spelling(\"несуществующееслово\"))  \n",
    "text = \"Ты тупкя!\"\n",
    "\n",
    "corrected = checker.correct_text(text)\n",
    "print(corrected)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0.1\n"
     ]
    }
   ],
   "source": [
    "import flask_cors\n",
    "print(flask_cors.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "mat_regex = r\"\"\"(?iux)(?<![а-яё])(?:\n",
    "(?:(?:у|[нз]а|(?:хитро|не)?вз?[ыьъ]|с[ьъ]|(?:и|ра)[зс]ъ?|(?:о[тб]|п[оа]д)[ьъ]?|(?:\\S(?=[а-яё]))+?[оаеи-])?[- ]*)?(?:\n",
    "  [её](?:б(?!о[рй]|рач)|[- ]*п[уа](?:[- ]*ц|[- ]*тс))|\n",
    "  [ий][- ]*[пб][- ]*[ае][- ]*[тцд][- ]*[ьъ]\n",
    ").*?|\n",
    "\n",
    "(?:(?:н[ийеа]|(?:ра|и)[зс]|[зд]?[ао](?:т|дн[оа])?|с(?:м[еий])?|а[пб]ч|в[ъы]?|пр[еий])[- ]*)?ху(?:[яйиеёю]|л+[ий](?!ган)).*?|\n",
    "\n",
    "бл(?:[эя]|еа?)(?:[дт][ьъ]?)?|\n",
    "\n",
    "\\S*?(?:\n",
    "  п(?:\n",
    "    [иеё]+[- ]*[зс]+[- ]*д|\n",
    "    [ий][- ]*д[аое]?[- ]*р|\n",
    "    е[- ]*д(?:[- ]*р(?!о)|[аое][- ]*р|[ий][- ]*к)|\n",
    "    о[- ]*х[- ]*у[- ]*ю\n",
    "  )|\n",
    "  бл[- ]*я(?:[дбц]|тс)|\n",
    "  [ое][- ]*х[- ]*у[яйиеё]|\n",
    "  х[- ]*у[- ]*[йи][- ]*н\n",
    ").*?|\n",
    "\n",
    "(?:о[тб]?|про|на|вы)?м(?:\n",
    "  а[- ]*н[- ]*д(?:[ауеыи](?:л(?:[ий][сзщ])?[ауеиы])?|ой|[ао]в.*?|юк(?:ов|[ауи])?|е[нт]ь|[ий]ща)|\n",
    "  у[- ]*д(?:[яаиое].+?|е?н(?:[ьюия]|ей))|\n",
    "  [ао][- ]*л[ао][- ]*ф[ьъ](?:[яиюе]|[еёо][йи])\n",
    ")|\n",
    "\n",
    "е[- ]*л[- ]*д[ауые].*?|\n",
    "л[- ]*я[тд][- ]*ь|\n",
    "(?:[нз]а|по)х|\n",
    "\n",
    "(?:г[- ]*а[- ]*н[- ]*д[- ]*о[- ]*н[а-яё]*|м[- ]*о[- ]*з[- ]*г[- ]*о[- ]*[её][- ]*б[а-яё]*|х[- ]*у[- ]*[йи][- ]*л[- ]*о[а-яё]*|з[- ]*а[- ]*л[- ]*у[- ]*п[- ]*а[а-яё]*|\n",
    "п(?:[- ]*[ий][- ]*з[- ]*д[её]?ц|[- ]*з[- ]*д[- ]*ц)|в[- ]*ы[- ]*п[- ]*[ий][- ]*з[- ]*д[а-яё]*|м[- ]*у[- ]*д[- ]*а[- ]*к[а-яё]*|е[- ]*б[- ]*л[- ]*а[а-яё]*|\n",
    "ш[- ]*а[- ]*л[- ]*а[- ]*в[а-яё]*|д[- ]*е[- ]*р[- ]*ь[- ]*м[- ]*о[- ]*е[- ]*д[а-яё]*|с[- ]*к[- ]*о[- ]*т[- ]*о[- ]*б[- ]*л[- ]*ю[- ]*д[а-яё]*|\n",
    "ч[- ]*м[- ]*о[- ]*ш[- ]*н[- ]*[ий][- ]*к[а-яё]*|п[- ]*р[- ]*о[- ]*с[- ]*т[- ]*о[- ]*ф[- ]*[ий][- ]*л[а-яё]*)|\n",
    "\n",
    "# 1. Общие оскорбления\n",
    "(?:у[- ]*р[- ]*о[- ]*д[а-яё]*|д[- ]*е[- ]*б[- ]*[ий][лв][а-яё]*|д[- ]*а[- ]*у[- ]*н[а-яё]*|к[- ]*р[- ]*е[- ]*т[- ]*[ий][- ]*н[а-яё]*|[ий][- ]*д[ий][ао][- ]*т[а-яё]*|д[- ]*о[- ]*л[- ]*б[- ]*о[ёе][- ]*б[а-яё]*|\n",
    "п[- ]*р[- ]*[ий][- ]*д[- ]*у[- ]*р[а-яё]*|о[- ]*т[- ]*м[- ]*о[- ]*р[- ]*о[- ]*з[а-яё]*|г[- ]*н[- ]*[ий][- ]*д[а-яё]*|п[- ]*о[- ]*д[- ]*о[- ]*н[а-яё]*|о[- ]*т[- ]*б[- ]*р[- ]*о[- ]*с[а-яё]*|м[- ]*е[- ]*р[- ]*з[- ]*а[- ]*в[а-яё]*|\n",
    "н[- ]*е[- ]*г[- ]*о[- ]*д[- ]*я[йя][а-яё]*|п[- ]*о[- ]*д[- ]*л[- ]*е[- ]*ц[а-яё]*|[ий][- ]*с[- ]*ч[- ]*а[- ]*д[- ]*[ий][а-яё]*|ч[- ]*м[оа][а-яё]*|л[- ]*о[- ]*х[а-яё]*|п[- ]*а[- ]*д[- ]*л[ао][йея]|п[- ]*а[- ]*д[- ]*л[- ]*ю[кг][а-яё]*)|\n",
    "\n",
    "# 2. Животные-оскорбления\n",
    "(?:с[- ]*к[- ]*о[- ]*т[- ]*[ий][- ]*н[а-яё]*|п[- ]*с[- ]*[ий][- ]*н[а-яё]*|с[- ]*в[- ]*[ий][- ]*н[- ]*ь[яи][а-яё]*|о[- ]*в[- ]*ц[аеы][а-яё]*|к[- ]*о[- ]*з[ёе][- ]*л[а-яё]*|б[- ]*з[- ]*д[а-яё]*)|\n",
    "\n",
    "# 3. Сексуально-оскорбительные и разврат\n",
    "(?:ш[- ]*л[- ]*ю[- ]*х[а-яё]*|п[- ]*р[- ]*о[- ]*с[- ]*т[- ]*и[- ]*т[- ]*у[- ]*т[- ]*к[а-яё]*|б[- ]*л[- ]*у[- ]*д[- ]*н[- ]*[ий][- ]*ц[а-яё]*|р[- ]*а[- ]*з[- ]*в[- ]*р[- ]*а[- ]*т[- ]*н[- ]*[ий][- ]*к[а-яё]*|п[- ]*о[- ]*х[- ]*о[- ]*т[а-яё]*|б[- ]*л[- ]*я[- ]*д[а-яё]*|\n",
    "с[- ]*о[- ]*д[- ]*о[- ]*м[а-яё]*|г[- ]*о[- ]*м[- ]*о[- ]*с[- ]*е[- ]*к[а-яё]*|п[- ]*е[- ]*д[- ]*о[- ]*ф[- ]*[ий][- ]*л[а-яё]*|[ий][- ]*з[- ]*в[- ]*р[- ]*а[- ]*щ[- ]*е[- ]*н[а-яё]*|п[- ]*р[ийо][- ]*с[- ]*т[ийо][- ]*т[а-яё]*|с[- ]*е[- ]*к[- ]*с[- ]*о[- ]*т[а-яё]*|\n",
    "т[- ]*р[- ]*а[- ]*х[а-яё]*|е[- ]*б[а-яё]*|п[- ]*о[- ]*р[- ]*н[- ]*о[а-яё]*|в[- ]*а[- ]*г[- ]*[ий][- ]*н[а-яё]*|ч[- ]*л[- ]*е[- ]*м[а-яё]*|с[- ]*о[- ]*с[аи][а-яё]*|ж[- ]*[оа][- ]*п[а-яё]*)\n",
    "\n",
    "# 4. Алкоголь и наркотики\n",
    "(?:а[- ]*л[- ]*к[- ]*а[- ]*ш[а-яё]*|п[- ]*ь[- ]*я[- ]*н[- ]*ь[а-яё]*|б[- ]*у[- ]*х[- ]*л[ао][а-яё]*|н[- ]*а[- ]*р[- ]*к[- ]*о[- ]*м[- ]*а[- ]*н[а-яё]*|д[- ]*о[- ]*з[а-яё]*|г[- ]*е[- ]*р[- ]*о[- ]*[ий][- ]*н[а-яё]*|\n",
    "к[- ]*о[- ]*к[- ]*с[а-яё]*|а[- ]*н[- ]*а[- ]*ш[а-яё]*|с[- ]*п[- ]*а[- ]*[ий][- ]*с[а-яё]*|ш[- ]*[ий][- ]*р[а-яё]*|т[- ]*о[- ]*р[- ]*ч[а-яё]*|д[- ]*р[- ]*ю[- ]*ч[а-яё]*|к[- ]*о[- ]*л[ёе][- ]*ц[а-яё]*|\n",
    "м[- ]*а[- ]*р[- ]*[ий][- ]*х[- ]*у[- ]*а[- ]*н[а-яё]*|г[- ]*а[- ]*ш[а-яё]*|п[ао][- ]*р[- ]*н[а-яё]*|з[- ]*а[- ]*к[- ]*л[- ]*а[- ]*д[а-яё]*|д[- ]*р[- ]*я[- ]*н[- ]*ь[а-яё]*)|\n",
    "\n",
    "# 5. Социальные оскорбления\n",
    "(?:б[- ]*о[- ]*м[- ]*ж[а-яё]*|п[- ]*а[- ]*р[- ]*а[- ]*з[- ]*[ий][- ]*т[а-яё]*|ш[- ]*а[- ]*р[- ]*о[- ]*м[- ]*ы[- ]*ж[- ]*н[- ]*[ий][- ]*к[а-яё]*|у[- ]*б[- ]*л[- ]*ю[- ]*д[а-яё]*|в[- ]*ы[- ]*б[- ]*л[- ]*я[- ]*д[а-яё]*)|\n",
    "\n",
    "# 6. \"Грязные\" слова\n",
    "(?:[гж][- ]*а[- ]*[вф][- ]*н[аоы]|г[ао][- ]*в[- ]*н[аоу]|г[- ]*о[- ]*в[- ]*е[- ]*н[а-яё]*|г[- ]*а[- ]*в[- ]*н[- ]*ю[кч]|д[- ]*е[- ]*р[- ]*ь[- ]*м[аоуы]|д[- ]*е[- ]*р[ьъ][- ]*м[аоу]|д[еи][- ]*р[- ]*ь[- ]*м[а-яё]*)|\n",
    "\n",
    "# 7. Оскорбления женского рода (обновлено)\n",
    "(?:с[ую][- ]*[кч][аиуео]|с[- ]*у[- ]*ч[аио][йея]|с[- ]*у[- ]*ч[- ]*к[аиу]|с[- ]*у[- ]*ч[- ]*ь[еяи]|с[- ]*т[- ]*е[- ]*р[- ]*в[а-яё]*|х[- ]*р[- ]*ы[- ]*ч[а-яё]*|\n",
    "ш[- ]*м[- ]*а[- ]*р[а-яё]*|ш[- ]*в[- ]*а[- ]*л[- ]*ь|ш[- ]*л[- ]*ю[- ]*х[а-яё]*|б[- ]*л[- ]*я[- ]*д[а-яё]*|п[- ]*о[- ]*т[- ]*а[- ]*с[- ]*к[а-яё]*|п[- ]*р[оа][- ]*л[её][- ]*т[- ]*к[а-яё]*)|\n",
    "\n",
    "# 8. Другие\n",
    "(?:х[- ]*а[- ]*м[а-яё]*|х[- ]*а[- ]*м[- ]*л[ао][а-яё]*|м[- ]*р[- ]*а[- ]*з[аоуи]|м[- ]*р[- ]*а[- ]*з[- ]*[ий]|м[- ]*р[- ]*а[- ]*з[- ]*о[- ]*т[а-яё]*|а[- ]*у[- ]*т[- ]*[ий][- ]*с[- ]*т[а-яё]*|д[- ]*е[- ]*г[- ]*е[- ]*н[- ]*е[- ]*р[- ]*а[- ]*т[а-яё]*)\n",
    ")(?![а-яё])\"\"\"\n",
    "\n",
    "is_mat = bool(re.search(mat_regex, \"проститутка\", flags=re.VERBOSE))\n",
    "print(is_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "чернуха: True\n"
     ]
    }
   ],
   "source": [
    "mat_regex = r\"\"\"(?iux)(?<![а-яё])(?:\n",
    "# Исходные матерные паттерны (с поддержкой дефисов и пробелов)\n",
    "(?:(?:у|[- ]*[нз][- ]*а|(?:х[- ]*и[- ]*т[- ]*р[- ]*о|н[- ]*е)?[- ]*в[- ]*з?[- ]*[ыьъ]|[- ]*с[- ]*[ьъ]|(?:и|р[- ]*а)[- ]*[зс][- ]*ъ?|(?:о[- ]*[тб]|п[- ]*[оа][- ]*д)[- ]*[ьъ]?|(?:\\S(?=[а-яё]))+?[- ]*[оаеи][- ]*)[- ]*)?(?:\n",
    "  [- ]*[её][- ]*(?:[- ]*б(?![- ]*о[- ]*[рй]|[- ]*р[- ]*а[- ]*ч)|[- ]*п[- ]*[уа][- ]*(?:[- ]*ц|[- ]*т[- ]*с))|\n",
    "  [- ]*и[- ]*[пб][- ]*[ае][- ]*[тцд][- ]*[ьъ]\n",
    ").*?|\n",
    "\n",
    "(?:(?:н[- ]*[иеа]|(?:р[- ]*а|и)[- ]*[зс]|[- ]*[зд][- ]*?[ао][- ]*(?:т|д[- ]*н[- ]*[оа])?|[- ]*с[- ]*(?:м[- ]*[еи])?|[- ]*а[- ]*[пб][- ]*ч|[- ]*в[- ]*[ъы]?|[- ]*п[- ]*р[- ]*[еи])[- ]*)?х[- ]*у(?:[- ]*[яйиеёю]|[- ]*л+[- ]*и(?!г[- ]*а[- ]*н)).*?|\n",
    "\n",
    "б[- ]*л(?:[- ]*[эя]|[- ]*е[- ]*а?)(?:[- ]*[дт][- ]*[ьъ]?)?|\n",
    "\n",
    "\\S*?(?:\n",
    "  п[- ]*(?:\n",
    "    (?:[- ]*[иеё][- ]*[зс][- ]*д|[- ]*з[- ]*д)[- ]*(?:[- ]*[еёц])? |  \n",
    "    [- ]*[иеё][- ]*з[- ]*д|  \n",
    "    [- ]*и[- ]*д[- ]*[аое][- ]*?р|\n",
    "    [- ]*е[- ]*д(?:[- ]*р(?![- ]*о)|[- ]*[аое][- ]*р|[- ]*и[- ]*к)|\n",
    "    [- ]*о[- ]*х[- ]*у[- ]*ю\n",
    "  )|\n",
    "  б[- ]*л[- ]*я(?:[- ]*[дбц]|[- ]*т[- ]*с)|\n",
    "  [- ]*[ое][- ]*х[- ]*у[- ]*[яйиеё]|\n",
    "  х[- ]*у[- ]*й[- ]*н\n",
    ").*?|\n",
    "\n",
    "(?:о[- ]*[тб]?|п[- ]*р[- ]*о|н[- ]*а|в[- ]*ы)?м[- ]*(?:\n",
    "  а[- ]*н[- ]*д(?:[- ]*[ауеыий](?:[- ]*л(?:[- ]*и[- ]*[сзщ])?[- ]*[ауеийы])?|[- ]*о[- ]*й|[- ]*[ао][- ]*в.*?|[- ]*ю[- ]*к(?:[- ]*о[- ]*в|[- ]*[ауий])?|[- ]*е[- ]*[нт][- ]*ь|[- ]*и[- ]*щ[- ]*а)|\n",
    "  у[- ]*д(?:[- ]*[яаийое].+?|[- ]*е?[- ]*н(?:[- ]*[ьюийя]|[- ]*е[- ]*й))|\n",
    "  [- ]*[ао][- ]*л[- ]*[ао][- ]*ф[- ]*[ьъ](?:[- ]*[яийюе]|[- ]*[её][- ]*о[- ]*й)\n",
    ")|\n",
    "\n",
    "е[- ]*л[- ]*д[- ]*[ауые].*?|\n",
    "л[- ]*я[- ]*[тд][- ]*ь|\n",
    "(?:[- ]*[нз][- ]*а|п[- ]*о)[- ]*х|\n",
    "\n",
    "# Грубые/оскорбительные слова (не мат)\n",
    "# 1. Общие оскорбления\n",
    "(?:у[- ]*р[- ]*о[- ]*д[- ]*[а-яё-]*|д[- ]*е[- ]*б[- ]*и[- ]*[лчв][- ]*[а-яё-]*|д[- ]*а[- ]*у[- ]*н[- ]*[а-яё-]*|к[- ]*р[- ]*е[- ]*т[- ]*[ий][- ]*н[- ]*[а-яё-]*|[ий][- ]*д[- ]*[ио][- ]*[ао][- ]*т[- ]*[а-яё-]*|д[- ]*о[- ]*л[- ]*б[- ]*о[- ]*[ёе][- ]*б[- ]*[а-яё-]*|\n",
    "п[- ]*р[- ]*[ий][- ]*д[- ]*у[- ]*р[- ]*[а-яё-]*|о[- ]*т[- ]*м[- ]*о[- ]*р[- ]*о[- ]*з[- ]*[а-яё-]*|г[- ]*н[- ]*и[- ]*д[- ]*[а-яё-]*|п[- ]*о[- ]*д[- ]*о[- ]*н[- ]*[а-яё-]*|о[- ]*т[- ]*б[- ]*р[- ]*о[- ]*с[- ]*[а-яё-]*|м[- ]*е[- ]*р[- ]*з[- ]*а[- ]*в[- ]*[а-яё-]*|\n",
    "н[- ]*е[- ]*г[- ]*о[- ]*д[- ]*я[- ]*[йя][- ]*[а-яё-]*|п[- ]*о[- ]*д[- ]*л[- ]*е[- ]*ц[- ]*[а-яё-]*|[ий][- ]*с[- ]*ч[- ]*а[- ]*д[- ]*[ий][- ]*[а-яё-]*|ч[- ]*м[- ]*[оыа][- ]*[а-яё-]*|л[- ]*о[- ]*х[- ]*[а-яё-]*|\n",
    "п[- ]*а[- ]*д[- ]*л[- ]*[уайея][- ]*|п[- ]*а[- ]*д[- ]*л[- ]*ю[- ]*[кг][- ]*[а-яё-]*|и[- ]*м[- ]*б[- ]*и[- ]*ц[- ]*и[- ]*л[- ]*[а-яё-]*|\n",
    "д[- ]*у[- ]*р[- ]*[а-яё]*|м[- ]*у[- ]*д(?:[а-яё-]+[а-яё-]|[- ]*[^а-яё-])[а-яё-]*|х[- ]*е[- ]*р[- ]*[а-яё-]*)|\n",
    "\n",
    "# 2. Животные-оскорбления\n",
    "(?:с[- ]*к[- ]*о[- ]*т[- ]*[ий][- ]*н[- ]*[а-яё-]*|п[- ]*с[- ]*[ий][- ]*н[- ]*[а-яё-]*|с[- ]*в[- ]*[ий][- ]*н[- ]*ь[- ]*[яий][- ]*[а-яё-]*|о[- ]*в[- ]*ц[- ]*[аеы][- ]*[а-яё-]*|к[- ]*о[- ]*з[- ]*[ёе][- ]*л[- ]*[а-яё-]*|б[- ]*з[- ]*д[- ]*[а-яё-]*|\n",
    "о[- ]*б[- ]*о[- ]*р[- ]*м[- ]*о[- ]*т[- ]*[а-яё-]*)|\n",
    "\n",
    "# 3. Сексуально-оскорбительные и разврат\n",
    "(?:ш[- ]*л[- ]*ю[- ]*х[- ]*[а-яё-]*|п[- ]*р[- ]*о[- ]*с[- ]*т[- ]*[ий][- ]*т[- ]*у[- ]*[а-яё-]*|б[- ]*л[- ]*у[- ]*д[- ]*н[- ]*[ий][- ]*ц[- ]*[а-яё-]*|р[- ]*а[- ]*з[- ]*в[- ]*р[- ]*а[- ]*т[- ]*н[- ]*[ий][- ]*к[- ]*[а-яё-]*|п[- ]*о[- ]*х[- ]*о[- ]*т[- ]*[а-яё-]*|б[- ]*л[- ]*я[- ]*д[- ]*[а-яё-]*|\n",
    "с[- ]*о[- ]*д[- ]*о[- ]*м[- ]*[а-яё-]*|г[- ]*о[- ]*м[- ]*о[- ]*с[- ]*е[- ]*к[- ]*[а-яё-]*|п[- ]*е[- ]*д[- ]*о[- ]*ф[- ]*[ий][- ]*л[- ]*[а-яё-]*|и[- ]*з[- ]*в[- ]*р[- ]*а[- ]*щ[- ]*е[- ]*н[- ]*[а-яё-]*|п[- ]*р[- ]*[ио][- ]*с[- ]*т[- ]*[ио][- ]*т[- ]*[а-яё-]*|с[- ]*е[- ]*к[- ]*с[- ]*о[- ]*т[- ]*[а-яё-]*|\n",
    "т[- ]*р[- ]*а[- ]*х[- ]*[а-яё]*|е[- ]*б[- ]*[а-яё-]*|п[- ]*о[- ]*р[- ]*н[- ]*о[- ]*[а-яё-]*|в[- ]*а[- ]*г[- ]*[ий][- ]*н[- ]*[а-яё-]*|ч[- ]*л[- ]*е[- ]*н[- ]*[а-яё-]*|[а-яё-]*с[- ]*о[- ]*с[- ]*[аий][- ]*[а-яё-]*|[а-яё-]*ж[- ]*о[- ]*п[- ]*[а-яё-]*|б[- ]*л[- ]*у[- ]*д[- ]*[а-яё-]*|б[- ]*л[- ]*у[- ]*д[- ]*н[- ]*[ий][- ]*к[- ]*[а-яё-]*|\n",
    "м[- ]*[ий][- ]*н[- ]*е[- ]*т[- ]*[а-яё-]*|г[- ]*а[- ]*н[- ]*д[- ]*о[- ]*н[- ]*[а-яё-]*|м[- ]*[ий][- ]*н[- ]*ь[- ]*е[- ]*т[- ]*|п[- ]*е[- ]*н[- ]*и[- ]*с[- ]*[а-яё-]*|[а-яё-]*з[- ]*а[- ]*л[- ]*у[- ]*п[- ]*[а-яё-]*)|\n",
    "\n",
    "# 4. Алкоголь и наркотики\n",
    "(?:а[- ]*л[- ]*к[- ]*а[- ]*ш[- ]*[а-яё-]*|п[- ]*ь[- ]*я[- ]*н[- ]*[а-яё-]*|б[- ]*у[- ]*х[- ]*л[- ]*[ао][- ]*[а-яё-]*|н[- ]*а[- ]*р[- ]*к[- ]*о[- ]*м[- ]*а[- ]*н[- ]*[а-яё-]*|д[- ]*о[- ]*з[- ]*[а-яё-]*|г[- ]*е[- ]*р[- ]*о[- ]*(?:[ий](?!ня)[- ]*н[- ]*[а-яё-]*|й[- ]*н[- ]*[а-яё-]*)|\n",
    "к[- ]*о[- ]*к[- ]*с[- ]*[а-яё-]*|а[- ]*н[- ]*а[- ]*ш[- ]*[а-яё-]*|с[- ]*п[- ]*а[- ]*[ий][- ]*с[- ]*[а-яё-]*|ш[- ]*[ий][- ]*р[- ]*[а-яё-]*|т[- ]*о[- ]*р[- ]*ч[- ]*[а-яё-]*|д[- ]*р[- ]*ю[- ]*ч[- ]*[а-яё-]*|к[- ]*о[- ]*л[- ]*[ёе][- ]*ц[- ]*[а-яё-]*|\n",
    "м[- ]*а[- ]*р[- ]*[ий][- ]*х[- ]*у[- ]*а[- ]*н[- ]*[а-яё-]*|г[- ]*а[- ]*ш[- ]*[а-яё-]*|п[- ]*[ао][- ]*р[- ]*н[- ]*[а-яё-]*|з[- ]*а[- ]*к[- ]*л[- ]*а[- ]*д[- ]*[а-яё-]*|д[- ]*р[- ]*я[- ]*н[- ]*ь[- ]*[а-яё-]*|\n",
    "[а-яё-]*д[- ]*р[- ]*о[- ]*ч[- ]*[а-яё-]*)|\n",
    "\n",
    "# 5. Социальные оскорбления\n",
    "(?:б[- ]*о[- ]*м[- ]*ж[- ]*[а-яё-]*|п[- ]*а[- ]*р[- ]*а[- ]*з[- ]*[ий][- ]*т[- ]*[а-яё-]*|ш[- ]*а[- ]*р[- ]*о[- ]*м[- ]*ы[- ]*ж[- ]*н[- ]*[ий][- ]*к[- ]*[а-яё-]*|у[- ]*б[- ]*л[- ]*ю[- ]*д[- ]*[а-яё-]*|в[- ]*ы[- ]*б[- ]*л[- ]*я[- ]*д[- ]*[а-яё-]*|б[- ]*и[- ]*ч[- ]*[а-яё-]*)|\n",
    "\n",
    "# 6. \"Грязные\" слова\n",
    "(?:[гж][- ]*а[- ]*[вф][- ]*н[- ]*[аоы]|[а-яё-]*г[- ]*[ао][- ]*в[- ]*н[- ]*[а-яё-]*[- ]*|г[- ]*[оа][- ]*в[- ]*е[- ]*н[- ]*[а-яё-]*|г[- ]*[ао][- ]*в[- ]*н[- ]*ю[- ]*[кч]|д[- ]*е[- ]*р[- ]*ь[- ]*м[- ]*[аоуы]|д[- ]*е[- ]*р[- ]*[ьъ][- ]*м[- ]*[аоу]|д[- ]*[еи][- ]*р[- ]*ь[- ]*м[- ]*[а-яё-]*)|\n",
    "\n",
    "# 7. Оскорбления женского рода (обновлено)\n",
    "(?:с[- ]*[ую][- ]*[кч][- ]*[аиуео]|с[- ]*у[- ]*ч[- ]*[аиьо][- ]*[йея]|с[- ]*у[- ]*ч[- ]*к[- ]*[аиу]|с[- ]*у[- ]*ч[- ]*ь[- ]*[еяий]|с[- ]*т[- ]*е[- ]*р[- ]*в[- ]*[а-яё-]*|х[- ]*р[- ]*ы[- ]*ч[- ]*[а-яё-]*|\n",
    "ш[- ]*м[- ]*а[- ]*р[- ]*[а-яё-]*|ш[- ]*в[- ]*а[- ]*л[- ]*[а-яё-]*|ш[- ]*л[- ]*ю[- ]*х[- ]*[а-яё-]*|б[- ]*л[- ]*я[- ]*д[- ]*[а-яё-]*|п[- ]*о[- ]*т[- ]*а[- ]*с[- ]*к[- ]*[а-яё-]*|п[- ]*р[- ]*[оа][- ]*л[- ]*[её][- ]*т[- ]*к[- ]*[а-яё-]*)|\n",
    "\n",
    "# Еще\n",
    "(?:д[- ]*а[- ]*в[- ]*а[- ]*л[- ]*к[- ]*[а-яё-]*|\n",
    "   п[- ]*е[- ]*т[- ]*у[- ]*[хш][- ]*[а-яё-]*|\n",
    "   р[- ]*у[- ]*к[- ]*о[- ]*б[- ]*л[- ]*у[- ]*д[- ]*[а-яё-]*|\n",
    "   [а-яё-]*с[- ]*с[- ]*а[- ]*[а-яё-]*|\n",
    "   в[- ]*л[- ]*а[- ]*г[- ]*а[- ]*л[- ]*и[- ]*щ[- ]*[а-яё-]*|\n",
    "   [а-яё-]*п[- ]*е[- ]*р[- ]*д[- ]*[а-яё-]*|\n",
    "   м[- ]*а[- ]*л[- ]*а[- ]*ф[- ]*ь[- ]*[а-яё-]*|\n",
    "   г[- ]*о[- ]*м[- ]*и[- ]*к[- ]*[а-яё-]*|\n",
    "   п[- ]*и[- ]*л[- ]*о[- ]*т[- ]*к[- ]*[а-яё-]*|\n",
    "   а[- ]*н[- ]*у[- ]*с[- ]*[а-яё-]*|\n",
    "   п[- ]*у[- ]*т[- ]*а[- ]*н[- ]*[ыа-]*[а-яё-]|\n",
    "   ш[- ]*а[- ]*л[- ]*а[- ]*в[- ]*[а-яё-]*|\n",
    "   м[- ]*о[- ]*ш[- ]*о[- ]*н[- ]*к[- ]*[а-яё-]*|\n",
    "   е[- ]*л[- ]*д[- ]*[ао][- ]*[а-яё-]*|\n",
    "   [а-яё-]*с[- ]*р[- ]*а[- ]*[тнцклч][- ]*[а-яё-]*|\n",
    "   [а-яё-]*д[- ]*р[- ]*[еи][- ]*[сщ][- ]*[а-яё-]*|\n",
    "   [а-яё-]*д[- ]*р[- ]*и[- ]*[сщ][- ]*[а-яё-]*|\n",
    "   с[- ]*п[- ]*[еи][- ]*[р][- ]*[м][- ]*[а-яё-]*|\n",
    "   д[- ]*[оа][- ]*[л][- ]*[б][- ]*[оа][- ]*[а-яё-]*|\n",
    "   [ао][- ]*[н][- ]*[а][- ]*[л][- ]*[ауомыь][- ]*[а-яё-]*|\n",
    "   [х][- ]*[р][- ]*[е][- ]*[н][- ]*[ьи][- ]*[а-яё-]*|\n",
    "   [а-яё-]*с[- ]*[еёи][- ]*р[- ]*|\n",
    "   [а-яё-]*с[- ]*[еёи][- ]*р[- ]*[с][- ]*[а-яё-]*|\n",
    "   [а-яё-]*с[- ]*[еёи][- ]*р[- ]*[аиес][- ]*[тцлеи][- ]*[а-яё-]*|\n",
    "   [оа][- ]*[л][- ]*[ие][- ]*[г][- ]*[оа][- ]*[ф][- ]*[р][- ]*[а-яё-]*|\n",
    "\t[ш][- ]*[аои][- ]*[п][- ]*[ао][- ]*[в][- ]*[ь][- ]*[а-яё-]*|\n",
    "    [ш][- ]*[п][- ]*[а][- ]*[к][- ]*[а-яё-]*|\n",
    "    [х][- ]*[оа][- ]*[х][- ]*[о][- ]*[л][- ]*[а-яё-]*|\n",
    "    [х][- ]*[оа][- ]*[х][- ]*[л][- ]*[а-яё-]*|\n",
    "    ж[- ]*л[- ]*о[- ]*б[- ]*[а-яё-]*|\n",
    "    т[- ]*в[- ]*[оа][- ]*р[- ]*[иь][- ]*[а-яё-]*|\n",
    "\tт[- ]*у[- ]*п[- ]*и[- ]*ц[- ]*[а-яё-]*|\n",
    "    в[- ]*ы[- ]*р[- ]*[ао][- ]*д[- ]*о[- ]*к[- ]*[а-яё-]*|\n",
    "    б[- ]*[оа][- ]*л[- ]*в[- ]*а[- ]*н[- ]*[а-яё-]*|\n",
    "    о[- ]*л[- ]*у[- ]*х[- ]*[а-яё-]*|\n",
    "    б[- ]*а[- ]*л[- ]*а[- ]*м[- ]*о[- ]*ш[- ]*к[- ]*[а-яё-]*|\n",
    "    ч[- ]*у[- ]*р[- ]*к[- ]*[а-яё-]*|\n",
    "    ж[- ]*и[- ]*д[- ]*[а-яё-]*|\n",
    "    к[- ]*а[- ]*ц[- ]*а[- ]*п[- ]*[а-яё-]*|\n",
    "    ч[- ]*у[- ]*х[- ]*а[- ]*н[- ]*[а-яё-]*|\n",
    "    п[- ]*[еи][- ]*н[- ]*д[- ]*о[- ]*с[- ]*[а-яё-]*|\n",
    "    б[- ]*а[- ]*н[- ]*д[- ]*е[- ]*р[- ]*[а-яё-]*|\n",
    "    в[- ]*а[- ]*т[- ]*н[- ]*и[- ]*к[- ]*[а-яё-]*|\n",
    "    б[- ]*и[- ]*о[- ]*м[- ]*а[- ]*с[- ]*с[- ]*[а-яё-]*|\n",
    "    [а-яё-]*м[- ]*у[- ]*с[- ]*[оа][- ]*[р][- ]*[а-яё-]*|\n",
    "    м[- ]*[еи][- ]*н[- ]*[т][- ]*[а-яё-]*|\n",
    "    б[- ]*и[- ]*о[- ]*о[- ]*т[- ]*х[- ]*о[- ]*д[- ]*[а-яё-]*|\n",
    "    [а-яё-]*м[- ]*о[- ]*ч[- ]*[аё][- ]*[а-яё-]*|\n",
    "    к[- ]*а[- ]*л[- ]*[а-яё-]*|\n",
    "    д[- ]*н[- ]*о[- ]*[а-яё-]*|\n",
    "    о[- ]*т[- ]*р[- ]*е[- ]*б[- ]*ь[- ]*[ея][- ]*[а-яё-]*|\n",
    "    т[- ]*р[- ]*о[- ]*л[- ]*[а-яё-]*|\n",
    "    х[- ]*е[- ]*й[- ]*т[- ]*[еи][- ]*р[- ]*[а-яё-]*|\n",
    "    к[- ]*л[- ]*о[- ]*у[- ]*н[- ]*[а-яё-]*|\n",
    "    ш[- ]*[аи][- ]*р[- ]*л[- ]*[ао][- ]*т[- ]*а[- ]*н[- ]*[а-яё-]*|\n",
    "    ш[- ]*[ие][- ]*з[- ]*[а-яё-]*|\n",
    "    ю[- ]*р[- ]*о[- ]*д[- ]*и[- ]*в[- ]*[ыий][- ]*[а-яё-]*|\n",
    "    у[- ]*[зс][- ]*к[- ]*[оа][- ]*г[- ]*л[- ]*а[- ]*з[- ]*[а-яё-]*|\n",
    "    з[- ]*а[- ]*д[- ]*н[- ]*[ие][- ]*ц[- ]*[а-яё-]*|\n",
    "    ч[- ]*у[- ]*ч[- ]*е[- ]*л[- ]*о[- ]*[а-яё-]*|\n",
    "    г[- ]*а[- ]*д[- ]*[еи][- ]*н[- ]*[а-яё-]*|\n",
    "    г[- ]*а[- ]*д[- ]*[а-яё-]*|\n",
    "    б[- ]*е[- ]*з[- ]*м[- ]*о[- ]*з[- ]*г[- ]*л[- ]*[а-яё-]*|\n",
    "    п[- ]*р[- ]*и[- ]*м[- ]*а[- ]*т[- ]*[а-яё-]*|\n",
    "    о[- ]*б[- ]*е[- ]*з[- ]*ь[- ]*я[- ]*н[- ]*а-яё-]*|\n",
    "    н[- ]*[еи][- ]*г[- ]*р[- ]*[а-яё-]*|\n",
    "    н[- ]*[еи][- ]*г[- ]*г[- ]*е[- ]*р[- ]*[а-яё-]*|\n",
    "    д[- ]*е[- ]*г[- ]*р[- ]*[ао][- ]*д[- ]*[а-яё-]*|\n",
    "    д[- ]*е[- ]*г[- ]*р[- ]*о[- ]*[еи][- ]*д[- ]*[а-яё-]*|\n",
    "    ж[- ]*и[- ]*р[- ]*т[- ]*р[- ]*е[- ]*с[- ]*[а-яё-]*|\n",
    "    д[- ]*р[- ]*ы[- ]*[хщ][- ]*[а-яё-]*|\n",
    "    н[- ]*и[- ]*щ[- ]*[еи][- ]*б[- ]*р[- ]*о[- ]*д[- ]*[а-яё-]*|\n",
    "    н[- ]*и[- ]*щ[- ]*[еи][- ]*[ё][- ]*б[- ]*[а-яё-]*|\n",
    "    м[- ]*а[- ]*к[- ]*а[- ]*к[- ]*[а-яё-]*|\n",
    "    ж[- ]*и[- ]*р[- ]*[оа][- ]*б[- ]*а[- ]*с[- ]*[а-яё-]*|\n",
    "    с[- ]*т[- ]*а[- ]*р[- ]*п[- ]*е[- ]*р[- ]*[а-яё-]*|\n",
    "    м[- ]*о[- ]*л[- ]*о[- ]*к[- ]*о[- ]*с[- ]*[а-яё-]*|\n",
    "    ч[- ]*[ие][- ]*р[- ]*н[- ]*у[- ]*х[- ]*[а-яё-]*|\n",
    "    \n",
    ")(?![а-яё])|\n",
    "\n",
    "# 8. Другие\n",
    "(?:х[- ]*а[- ]*м[- ]*[а-яё-]*|х[- ]*а[- ]*м[- ]*л[- ]*[ао][- ]*[а-яё-]*|м[- ]*р[- ]*а[- ]*з[- ]*[аоуий][- ]*|м[- ]*р[- ]*а[- ]*з[- ]*[ийь][- ]*|м[- ]*р[- ]*а[- ]*з[- ]*о[- ]*т[- ]*[а-яё-]*|а[- ]*у[- ]*т[- ]*[ий][- ]*с[- ]*т[- ]*[а-яё-]*|а[- ]*х[- ]*у[- ]*е[- ]*[а-яё-]*|д[- ]*е[- ]*г[- ]*е[- ]*н[- ]*е[- ]*р[- ]*а[- ]*т[- ]*[а-яё-]*|к[- ]*о[- ]*н[- ]*ч[- ]*[а-яё-]*)\n",
    ")(?![а-яё])\"\"\"\n",
    "word = \"чернуха\"\n",
    "print(f\"{word}: {bool(re.fullmatch(mat_regex, word, flags=re.VERBOSE))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
